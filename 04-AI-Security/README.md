# AI Security

AI and LLM security testing, prompt injection, and security vulnerabilities in AI systems.

## ðŸ“‹ Content

- **[AI Pentest](./ai.md)**: AI/LLM penetration testing methodologies

## ðŸŽ¯ Topics Covered

### OWASP LLM Top 10
1. Prompt Injection
2. Insecure Output Handling
3. Training Data Poisoning
4. Model Denial of Service
5. Supply Chain Vulnerabilities
6. Sensitive Information Disclosure
7. Insecure Plugin Design
8. Excessive Agency
9. Overreliance
10. Model Theft

### Testing Techniques
- Direct prompt injection
- Indirect prompt injection
- Jailbreaking techniques
- Tool/Plugin abuse
- RAG-specific attacks
- MLOps security

## ðŸ”— Resources

- [OWASP Top 10 for LLM Applications](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- [garak LLM Scanner](https://github.com/leondz/garak)
- [Microsoft PyRIT](https://github.com/Azure/PyRIT)
- [NeMo Guardrails](https://github.com/NVIDIA/NeMo-Guardrails)
